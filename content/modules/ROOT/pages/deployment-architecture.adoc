= Zero Touch Deployment Architecture

== Learning Outcomes

Upon completion, you will understand:

* Complete deployment pipeline from Git repository to running lab
* Role of each system component (AgnosticV, AgnosticD, OpenShift CNV, Showroom)
* Configuration transformation process and variable mapping between systems
* Deployment issue debugging by understanding where failures can occur in the pipeline
* Lab design optimization based on deployment architecture constraints and capabilities

== Overview

Understanding how your Zero Touch labs get deployed is crucial for advanced customization and troubleshooting. This guide explains the backend deployment process and integration patterns.

== Architecture Overview

Zero Touch labs use a sophisticated deployment pipeline that transforms your template repository into running lab environments:

```
[Your Lab Repo] → [AgnosticD] → [OpenShift CNV] → [Running Lab]
      ↓               ↓              ↓              ↓
   Template        Config        Infrastructure   Student
   Content         Engine        Deployment       Experience
```

=== Key Components

**AgnosticD**: The deployment engine that orchestrates infrastructure provisioning
**AgnosticV**: The catalog system that defines lab configurations and metadata  
**Showroom**: The UI framework that presents your content to students
**OpenShift CNV**: The virtualization platform hosting your lab VMs

== Backend Repository Structure

Your lab template integrates with two backend systems:

=== AgnosticD Configuration
Located at: `agnosticd/ansible/configs/zero-touch-base-rhel/`

[source,yaml]
----
zero-touch-base-rhel/
├── default_vars_openshift_cnv.yaml    # CNV platform defaults
├── default_vars.yml                   # Base configuration
├── software.yml                       # Software installation
├── post_software.yml                  # Post-deployment tasks
└── README.adoc                        # Config documentation
----

**Purpose**: Defines how infrastructure is created, configured, and managed.

=== AgnosticV Catalog Entry  
Located at: `agnosticv_all/zt-rhelbu-agnosticv/zt-rhelbu/[your-lab]/`

[source,yaml]
----
your-lab/
├── common.yaml        # Shared configuration + Git integration
├── dev.yaml          # Development settings  
├── prod.yaml         # Production settings (chart v1.9.10)
├── test.yaml         # Testing configuration
└── description.adoc  # Catalog description
----

**Purpose**: Orchestrates deployment by transforming your Git repository into AgnosticD variables.

**Key Integration Logic**:
[source,yaml]
----
# Dynamic URL transformation (common.yaml)
git_config_directory: |-
  {{ ocp4_workload_showroom_content_git_repo  |
    replace('.git','/refs/heads/' + ocp4_workload_showroom_content_git_repo_ref + '/config/') |
    replace('github.com','raw.githubusercontent.com') }}

# Template file → AgnosticD variable mapping
containers: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml') | from_yaml).containers | default([]) }}"
instances: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml') | from_yaml).virtualmachines | default([]) }}"
zero_touch_ingress_lockdown_rules: "{{ (lookup('ansible.builtin.url', git_config_directory + 'firewall.yaml') | from_yaml).ingress | default([]) }}"
----

**URL Transformation Example**:
```
https://github.com/rhpds/zero_touch_template_wilson.git
      ↓
https://raw.githubusercontent.com/rhpds/zero_touch_template_wilson/refs/heads/main/config/
      ↓
https://raw.githubusercontent.com/rhpds/zero_touch_template_wilson/refs/heads/main/config/instances.yaml
```

== Deployment Process

=== Phase 1: Configuration Resolution
The deployment system dynamically fetches your lab configuration:

[source,yaml]
----
# From AgnosticV common.yaml
git_config_directory: |-
  {{ ocp4_workload_showroom_content_git_repo |
    replace('.git','/refs/heads/' + ref + '/config/') |
    replace('github.com','raw.githubusercontent.com') }}

# Dynamic configuration loading
instances: "{{ lookup('url', git_config_directory + 'instances.yaml') | from_yaml }}"
networks: "{{ lookup('url', git_config_directory + 'networks.yaml') | from_yaml }}"
firewall: "{{ lookup('url', git_config_directory + 'firewall.yaml') | from_yaml }}"
----

**What This Means**: Your `config/` directory files are fetched directly from your Git repository during deployment.

=== Phase 2: Infrastructure Provisioning
AgnosticD uses your configuration to create:

**Virtual Machines**: Based on your `instances.yaml`
**Networks**: Defined in `networks.yaml`  
**Firewall Rules**: From `firewall.yaml`
**Security Groups**: Automatically configured for isolation

=== Phase 3: Software Installation
The platform executes comprehensive role-based configuration:

**Pre-Software Roles**:
- `create_ssh_provision_key` - Generates SSH keys for deployment
- `set-repositories` - Configures RHEL repositories (Satellite integration)
- `common` - Installs common packages from your `instances.yaml`
- `bastion-base` - Configures bastion host essentials
- `zero_touch_rhel_user` - Creates the `rhel` lab user
- `asset_injector` - Injects custom assets (if configured)

**Software Installation**:
- Installs packages defined in your `instances.yaml` per VM/container
- Executes configurable `software_workloads` (extensions available)

**Available Workload Extensions**:
- `ocp4_workload_codeserver` - VS Code browser-based IDE
- `ocp4_workload_gitea_operator` - Git repository hosting
- `ocp4_workload_jenkins` - CI/CD pipeline integration
- `ocp4_workload_quay_operator` - Container registry
- Plus 200+ additional workloads for specialized requirements

=== Phase 4: Showroom Deployment
Your content is packaged and deployed via Helm:

**Showroom Helm Chart Deployment**:
- **Dev**: `showroom-single-pod v1.9.6` (development environments)
- **Prod**: `zerotouch v1.9.10` (production environments)
- **UI Bundle**: `nookbag-v0.0.5` (Zero Touch UI framework)

**Content Integration**:
- Antora processes your `site.yml` playbook
- Content served via `showroom-content:v1.2` image (prod)
- Dynamic Git repository content fetching

**Terminal Integration**:
- Wetty SSH terminals: `quay.io/rhpds/wetty`
- Auto-SSH to bastion: `ssh rhel@{{ groups['bastions'][0] }}`
- Environment variables: `guid`, `domain`, `common_password`

**URL Generation**: Creates unique URLs for each lab instance
- Application routes: `https://satellite-${guid}.${domain}/` (example from satellite lab)
- Container routes: `https://control-${guid}.${domain}/` (example from AAP lab)
- Service-specific: `https://node03-${guid}.${domain}/index.html` (example from roadshow)

== Key Integration Points

=== Git Repository Integration

Your repository structure directly maps to deployment configuration:

[source,yaml]
----
# Your repo structure
your-lab/
├── config/           → Fetched during deployment
├── content/          → Built into Antora site  
├── setup-automation/ → Executed during provisioning
├── runtime-automation/ → Available during lab execution
├── site.yml          → Antora configuration
└── ui-config.yml     → Showroom UI settings
----

=== Password Management

The platform generates secure passwords automatically:

[source,yaml]
----
# From AgnosticD
common_password: >-
  {{
    lookup('password', output_dir ~ '/common_password length=12 chars=ascii_letters,digits')
  }}

# Student access
student_password: "{{ common_password }}"
ansible_service_account_user_password: "{{ common_password }}"
----

** Security**: Each deployment gets a unique, randomly generated password.

=== Environment Variables

Your content has access to deployment-specific variables:

[source,yaml]
----
# Available in your templates
guid: "{{ guid }}"                    # Unique deployment ID
domain: "{{ sandbox_openshift_apps_domain }}" # Platform domain
common_password: "{{ common_password }}"      # Generated password
----

**Usage**: Use these in your content with `{guid}`, `{domain}`, etc.

== Platform Features

=== OpenShift CNV Integration

Your VMs run on OpenShift Container Native Virtualization:

** Features**:
- **Kubernetes-native VM management**
- **Automatic scheduling and resource management**  
- **Network isolation between lab deployments**
- **Persistent storage for VM disks**

=== Showroom UI Framework

The student interface provides:

**Navigation**: Multi-module content organization
**Terminals**: Browser-based SSH access to your VMs
**Responsive**: Works on desktop and mobile devices
**Customizable**: Configurable tabs, solve buttons, external links

== Deployment Metadata

=== Babylon/AgnosticV Integration

Your labs integrate with Red Hat's lab catalog system:

[source,yaml]
----
__meta__:
  catalog:
    namespace: babylon-catalog-prod
    display_name: "Your Lab Name"
    category: Workshops
    keywords:
      - rhel
      - zero-touch
  deployer:
    execution_environment:
      image: quay.io/agnosticd/ee-multicloud:v1.2
----

=== Resource Management

**Lifespan**: Labs have configurable runtime limits
**Access Control**: Integration with Red Hat SSO
**Reporting**: Usage analytics and cost tracking
**Quotas**: Resource limits per user/organization

== Advanced Configuration

=== Custom Execution Environments

For specialized deployments:

[source,yaml]
----
__meta__:
  deployer:
    execution_environment:
      image: quay.io/your-org/custom-ee:latest
      pull: missing
----

=== Network Customization

Advanced networking features:

[source,yaml]
----
# Custom ingress/egress rules
zero_touch_ingress_lockdown_rules:
  - from:
      - ipBlock:
          cidr: "10.0.0.0/8"
    ports:
      - protocol: TCP
        port: 8080

zero_touch_egress_lockdown_rules:
  - ports:
      - protocol: TCP
        port: 443
    to: []  # Allow HTTPS everywhere
----

==== Network Policy for Container SSH Access

**Critical for containers needing SSH access to VMs:**

[IMPORTANT]
====
**Default Network Policy Behavior**

Zero Touch deployments implement strict network policies for security:

- **Showroom pods** can SSH to VMs (default)
- **Custom containers** are blocked from SSH to VMs (security feature)
- **Container-to-container** communication works normally
- **VM-to-VM** communication works normally
====

**Required Configuration for SSH-enabled containers:**

[source,yaml]  
----
# Enable SSH access from specific containers to VMs
zero_touch_ingress_lockdown_rules:
  - from:
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: showroom  # Default Showroom access
  - from:  
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: vscode    # VS Code container
    ports:
      - protocol: TCP
        port: 22
  - from:  
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: monitoring  # Monitoring container
    ports:
      - protocol: TCP
        port: 22
----

**Key Implementation Details:**

* **Pod Labels**: Containers get `app.kubernetes.io/name: <container-name>` labels automatically
* **Namespace Scope**: Network policies apply within the CNV namespace
* **Security Isolation**: VMs and containers are in same namespace but isolated by network policy
* **Default Lockdown**: `lock_bastion_security_group_openshift_cnv.yml` applies the restrictions

**Common Use Cases:**
- Development environments (VS Code, IDEs) 
- Administrative containers (backup, deployment tools)
- Monitoring systems (node agents, log collectors)
- CI/CD containers (automation, testing tools)

**Network Architecture:**
- **Showroom namespace**: `showroom-{{ guid }}` (UI layer only - separate)
- **CNV namespace**: `{{ env_type }}-{{ guid }}` (ALL lab infrastructure: VMs + containers + services + routes)
- **Cross-namespace SSH**: Showroom → CNV VMs allowed (default policy rule)
- **Intra-namespace SSH**: Container → VM blocked by default (MORE restrictive, requires explicit policy)

[IMPORTANT]
====
**Critical Architecture Correction**

Containers and VMs deploy to the **SAME CNV namespace**, not separate namespaces. This makes network policy configuration **MORE critical** because:

* No natural namespace isolation between containers and VMs
* Network policies apply **within** the shared lab namespace
* Container SSH access requires explicit policy exceptions
* More restrictive than traditional cross-namespace security
====

== Troubleshooting

=== Common Deployment Issues

** Configuration Not Found**: Check your Git repository path and branch
** VM Creation Failed**: Verify your `instances.yaml` syntax
** Network Issues**: Review `firewall.yaml` and network policies
** Content Build Failed**: Validate `site.yml` Antora configuration

=== Debugging Tools

** AgnosticD Logs**: Available in deployment output directory
** OpenShift Console**: Monitor VM and pod status
** Bastion Access**: SSH to debug infrastructure issues
** Showroom Logs**: Container logs for UI troubleshooting

==  Related Documentation

* xref:template-customization-guide.adoc[Template Customization Guide]
* xref:advanced-lab-features.adoc[Advanced Lab Features]  
* xref:production-patterns-guide.adoc[Production Deployment Patterns]
* xref:enterprise-lab-patterns.adoc[Enterprise Lab Integration]

---
**Pro Tip**: Understanding the deployment architecture helps you design more efficient labs and troubleshoot issues quickly. The platform handles complexity so you can focus on creating great learning experiences.

[bibliography]
== References

* [[[agnosticd-base]]] Red Hat GPTE Team. AgnosticD Zero Touch Base RHEL Configuration. 
  `/home/wilson/Projects/agnosticd/ansible/configs/zero-touch-base-rhel/default_vars_openshift_cnv.yaml`. 2024.

* [[[roadshow-instances]]] Red Hat Ansible Team. AAP 2.5 Roadshow Lab Instance Configuration. 
  `/home/wilson/Projects/showroom_git/zt-ans-bu-roadshow01/config/instances.yaml`. 2024.

* [[[template-instances]]] Red Hat GPTE Team. Zero Touch Template Instance Configuration. 
  `/home/wilson/Projects/zero_touch_template_wilson/config/instances.yaml`. 2024.
