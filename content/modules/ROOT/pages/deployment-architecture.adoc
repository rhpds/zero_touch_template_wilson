= Zero Touch Deployment Architecture

== Learning Outcomes

By the end of this guide, you will:

* **Understand the complete deployment pipeline** from Git repository to running lab
* **Comprehend the role of each system component** (AgnosticV, AgnosticD, OpenShift CNV, Showroom)
* **Trace the configuration transformation process** and variable mapping between systems
* **Debug deployment issues** by understanding where failures can occur in the pipeline
* **Optimize lab designs** based on deployment architecture constraints and capabilities

== Overview

Understanding how your Zero Touch labs get deployed is crucial for advanced customization and troubleshooting. This guide explains the backend deployment process and integration patterns.

== ğŸ—ï¸ Architecture Overview

Zero Touch labs use a sophisticated deployment pipeline that transforms your template repository into running lab environments:

```
[Your Lab Repo] â†’ [AgnosticD] â†’ [OpenShift CNV] â†’ [Running Lab]
      â†“               â†“              â†“              â†“
   Template        Config        Infrastructure   Student
   Content         Engine        Deployment       Experience
```

=== Key Components

**ğŸ”§ AgnosticD**: The deployment engine that orchestrates infrastructure provisioning
**ğŸ“¦ AgnosticV**: The catalog system that defines lab configurations and metadata  
**ğŸ¨ Showroom**: The UI framework that presents your content to students
**â˜ï¸ OpenShift CNV**: The virtualization platform hosting your lab VMs

== ğŸ“ Backend Repository Structure

Your lab template integrates with two backend systems:

=== AgnosticD Configuration
Located at: `agnosticd/ansible/configs/zero-touch-base-rhel/`

[source,yaml]
----
zero-touch-base-rhel/
â”œâ”€â”€ default_vars_openshift_cnv.yaml    # CNV platform defaults
â”œâ”€â”€ default_vars.yml                   # Base configuration
â”œâ”€â”€ software.yml                       # Software installation
â”œâ”€â”€ post_software.yml                  # Post-deployment tasks
â””â”€â”€ README.adoc                        # Config documentation
----

**Purpose**: Defines how infrastructure is created, configured, and managed.

=== AgnosticV Catalog Entry  
Located at: `agnosticv_all/zt-rhelbu-agnosticv/zt-rhelbu/[your-lab]/`

[source,yaml]
----
your-lab/
â”œâ”€â”€ common.yaml        # Shared configuration + Git integration
â”œâ”€â”€ dev.yaml          # Development settings  
â”œâ”€â”€ prod.yaml         # Production settings (chart v1.9.10)
â”œâ”€â”€ test.yaml         # Testing configuration
â””â”€â”€ description.adoc  # Catalog description
----

**Purpose**: Orchestrates deployment by transforming your Git repository into AgnosticD variables.

**Key Integration Logic**:
[source,yaml]
----
# Dynamic URL transformation (common.yaml)
git_config_directory: |-
  {{ ocp4_workload_showroom_content_git_repo  |
    replace('.git','/refs/heads/' + ocp4_workload_showroom_content_git_repo_ref + '/config/') |
    replace('github.com','raw.githubusercontent.com') }}

# Template file â†’ AgnosticD variable mapping
containers: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml') | from_yaml).containers | default([]) }}"
instances: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml') | from_yaml).virtualmachines | default([]) }}"
zero_touch_ingress_lockdown_rules: "{{ (lookup('ansible.builtin.url', git_config_directory + 'firewall.yaml') | from_yaml).ingress | default([]) }}"
----

**URL Transformation Example**:
```
https://github.com/rhpds/zero_touch_template_wilson.git
      â†“
https://raw.githubusercontent.com/rhpds/zero_touch_template_wilson/refs/heads/main/config/
      â†“
https://raw.githubusercontent.com/rhpds/zero_touch_template_wilson/refs/heads/main/config/instances.yaml
```

== ğŸ”„ Deployment Process

=== Phase 1: Configuration Resolution
The deployment system dynamically fetches your lab configuration:

[source,yaml]
----
# From AgnosticV common.yaml
git_config_directory: |-
  {{ ocp4_workload_showroom_content_git_repo |
    replace('.git','/refs/heads/' + ref + '/config/') |
    replace('github.com','raw.githubusercontent.com') }}

# Dynamic configuration loading
instances: "{{ lookup('url', git_config_directory + 'instances.yaml') | from_yaml }}"
networks: "{{ lookup('url', git_config_directory + 'networks.yaml') | from_yaml }}"
firewall: "{{ lookup('url', git_config_directory + 'firewall.yaml') | from_yaml }}"
----

**ğŸ” What This Means**: Your `config/` directory files are fetched directly from your Git repository during deployment.

=== Phase 2: Infrastructure Provisioning
AgnosticD uses your configuration to create:

**ğŸ–¥ï¸ Virtual Machines**: Based on your `instances.yaml`
**ğŸŒ Networks**: Defined in `networks.yaml`  
**ğŸ”¥ Firewall Rules**: From `firewall.yaml`
**ğŸ”’ Security Groups**: Automatically configured for isolation

=== Phase 3: Software Installation
The platform executes comprehensive role-based configuration:

**ğŸ”§ Pre-Software Roles**:
- `create_ssh_provision_key` - Generates SSH keys for deployment
- `set-repositories` - Configures RHEL repositories (Satellite integration)
- `common` - Installs common packages from your `instances.yaml`
- `bastion-base` - Configures bastion host essentials
- `zero_touch_rhel_user` - Creates the `rhel` lab user
- `asset_injector` - Injects custom assets (if configured)

**ğŸ“¦ Software Installation**:
- Installs packages defined in your `instances.yaml` per VM/container
- Executes configurable `software_workloads` (extensions available)

**ğŸ”§ Available Workload Extensions**:
- `ocp4_workload_codeserver` - VS Code browser-based IDE
- `ocp4_workload_gitea_operator` - Git repository hosting
- `ocp4_workload_jenkins` - CI/CD pipeline integration
- `ocp4_workload_quay_operator` - Container registry
- Plus 200+ additional workloads for specialized requirements

=== Phase 4: Showroom Deployment
Your content is packaged and deployed via Helm:

**ğŸ¨ Showroom Helm Chart Deployment**:
- **Dev**: `showroom-single-pod v1.9.6` (development environments)
- **Prod**: `zerotouch v1.9.10` (production environments)
- **UI Bundle**: `nookbag-v0.0.5` (Zero Touch UI framework)

**ğŸ“š Content Integration**:
- Antora processes your `site.yml` playbook
- Content served via `showroom-content:v1.2` image (prod)
- Dynamic Git repository content fetching

**ğŸ”Œ Terminal Integration**:
- Wetty SSH terminals: `quay.io/rhpds/wetty`
- Auto-SSH to bastion: `ssh rhel@{{ groups['bastions'][0] }}`
- Environment variables: `guid`, `domain`, `common_password`

**ğŸŒ URL Generation**: Creates unique URLs for each lab instance
- Showroom UI: `https://showroom-{{ guid }}.{{ domain }}/`  
- Container routes: `https://{{ container-name }}-{{ guid }}.{{ domain }}/`

== ğŸ”§ Key Integration Points

=== Git Repository Integration

Your repository structure directly maps to deployment configuration:

[source,yaml]
----
# Your repo structure
your-lab/
â”œâ”€â”€ config/           â†’ Fetched during deployment
â”œâ”€â”€ content/          â†’ Built into Antora site  
â”œâ”€â”€ setup-automation/ â†’ Executed during provisioning
â”œâ”€â”€ runtime-automation/ â†’ Available during lab execution
â”œâ”€â”€ site.yml          â†’ Antora configuration
â””â”€â”€ ui-config.yml     â†’ Showroom UI settings
----

=== Password Management

The platform generates secure passwords automatically:

[source,yaml]
----
# From AgnosticD
common_password: >-
  {{
    lookup('password', output_dir ~ '/common_password length=12 chars=ascii_letters,digits')
  }}

# Student access
student_password: "{{ common_password }}"
ansible_service_account_user_password: "{{ common_password }}"
----

**ğŸ”’ Security**: Each deployment gets a unique, randomly generated password.

=== Environment Variables

Your content has access to deployment-specific variables:

[source,yaml]
----
# Available in your templates
guid: "{{ guid }}"                    # Unique deployment ID
domain: "{{ sandbox_openshift_apps_domain }}" # Platform domain
common_password: "{{ common_password }}"      # Generated password
----

**ğŸ’¡ Usage**: Use these in your content with `{guid}`, `{domain}`, etc.

== ğŸš€ Platform Features

=== OpenShift CNV Integration

Your VMs run on OpenShift Container Native Virtualization:

**ğŸ”§ Features**:
- **Kubernetes-native VM management**
- **Automatic scheduling and resource management**  
- **Network isolation between lab deployments**
- **Persistent storage for VM disks**

=== Showroom UI Framework

The student interface provides:

**ğŸ¯ Navigation**: Multi-module content organization
**ğŸ–¥ï¸ Terminals**: Browser-based SSH access to your VMs
**ğŸ“± Responsive**: Works on desktop and mobile devices
**ğŸ¨ Customizable**: Configurable tabs, solve buttons, external links

== ğŸ“Š Deployment Metadata

=== Babylon/AgnosticV Integration

Your labs integrate with Red Hat's lab catalog system:

[source,yaml]
----
__meta__:
  catalog:
    namespace: babylon-catalog-prod
    display_name: "Your Lab Name"
    category: Workshops
    keywords:
      - rhel
      - zero-touch
  deployer:
    execution_environment:
      image: quay.io/agnosticd/ee-multicloud:v1.2
----

=== Resource Management

**â±ï¸ Lifespan**: Labs have configurable runtime limits
**ğŸ”’ Access Control**: Integration with Red Hat SSO
**ğŸ“ˆ Reporting**: Usage analytics and cost tracking
**âš–ï¸ Quotas**: Resource limits per user/organization

== ğŸ› ï¸ Advanced Configuration

=== Custom Execution Environments

For specialized deployments:

[source,yaml]
----
__meta__:
  deployer:
    execution_environment:
      image: quay.io/your-org/custom-ee:latest
      pull: missing
----

=== Network Customization

Advanced networking features:

[source,yaml]
----
# Custom ingress/egress rules
zero_touch_ingress_lockdown_rules:
  - from:
      - ipBlock:
          cidr: "10.0.0.0/8"
    ports:
      - protocol: TCP
        port: 8080

zero_touch_egress_lockdown_rules:
  - ports:
      - protocol: TCP
        port: 443
    to: []  # Allow HTTPS everywhere
----

==== Network Policy for Container SSH Access

**Critical for containers needing SSH access to VMs:**

[IMPORTANT]
====
**Default Network Policy Behavior**

Zero Touch deployments implement strict network policies for security:

- **Showroom pods** can SSH to VMs (default)
- **Custom containers** are blocked from SSH to VMs (security feature)
- **Container-to-container** communication works normally
- **VM-to-VM** communication works normally
====

**Required Configuration for SSH-enabled containers:**

[source,yaml]  
----
# Enable SSH access from specific containers to VMs
zero_touch_ingress_lockdown_rules:
  - from:
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: showroom  # Default Showroom access
  - from:  
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: vscode    # VS Code container
    ports:
      - protocol: TCP
        port: 22
  - from:  
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: monitoring  # Monitoring container
    ports:
      - protocol: TCP
        port: 22
----

**Key Implementation Details:**

* **Pod Labels**: Containers get `app.kubernetes.io/name: <container-name>` labels automatically
* **Namespace Scope**: Network policies apply within the CNV namespace
* **Security Isolation**: VMs and containers are in same namespace but isolated by network policy
* **Default Lockdown**: `lock_bastion_security_group_openshift_cnv.yml` applies the restrictions

**Common Use Cases:**
- Development environments (VS Code, IDEs) 
- Administrative containers (backup, deployment tools)
- Monitoring systems (node agents, log collectors)
- CI/CD containers (automation, testing tools)

**Network Architecture:**
- **Showroom namespace**: `showroom-{{ guid }}` (UI layer only - separate)
- **CNV namespace**: `{{ env_type }}-{{ guid }}` (ALL lab infrastructure: VMs + containers + services + routes)
- **Cross-namespace SSH**: Showroom â†’ CNV VMs allowed (default policy rule)
- **Intra-namespace SSH**: Container â†’ VM blocked by default (MORE restrictive, requires explicit policy)

[IMPORTANT]
====
**Critical Architecture Correction**

Containers and VMs deploy to the **SAME CNV namespace**, not separate namespaces. This makes network policy configuration **MORE critical** because:

* No natural namespace isolation between containers and VMs
* Network policies apply **within** the shared lab namespace
* Container SSH access requires explicit policy exceptions
* More restrictive than traditional cross-namespace security
====

== ğŸ§° Troubleshooting

=== Common Deployment Issues

**âŒ Configuration Not Found**: Check your Git repository path and branch
**âŒ VM Creation Failed**: Verify your `instances.yaml` syntax
**âŒ Network Issues**: Review `firewall.yaml` and network policies
**âŒ Content Build Failed**: Validate `site.yml` Antora configuration

=== Debugging Tools

**ğŸ” AgnosticD Logs**: Available in deployment output directory
**ğŸ“Š OpenShift Console**: Monitor VM and pod status
**ğŸ› ï¸ Bastion Access**: SSH to debug infrastructure issues
**ğŸ“‹ Showroom Logs**: Container logs for UI troubleshooting

== ğŸ“š Related Documentation

* xref:template-customization-guide.adoc[Template Customization Guide]
* xref:advanced-lab-features.adoc[Advanced Lab Features]  
* xref:production-patterns-guide.adoc[Production Deployment Patterns]
* xref:enterprise-lab-patterns.adoc[Enterprise Lab Integration]

---
**ğŸ’¡ Pro Tip**: Understanding the deployment architecture helps you design more efficient labs and troubleshoot issues quickly. The platform handles complexity so you can focus on creating great learning experiences!
