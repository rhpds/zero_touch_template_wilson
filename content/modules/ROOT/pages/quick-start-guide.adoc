= Quick Start Guide for Internal Developers
:description: Essential Zero Touch patterns and configuration examples
:keywords: quick-start, patterns, naming-conventions, internal
:estimated-time: 10-15 minutes
:page-level: beginner
:page-role: quick-reference
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Overview

**Topics covered:**
* Zero Touch directory structure and naming conventions
* Configuration file patterns (instances.yaml, main.yml, ui-config.yml)
* UI tab patterns for services and terminals
* Enterprise examples from production labs

== Directory Structure & Naming Conventions

[%collapsible]
====
Understanding Zero Touch file relationships is critical for rapid development.

NOTE: The official template structure that learners will fork is available at https://github.com/rhpds/lab_zero_touch_template.git

.Zero Touch Template Structure (https://github.com/rhpds/lab_zero_touch_template.git)
[source,text]
----
lab_zero_touch_template/
├── config/
│   ├── instances.yaml              # <1>
│   ├── networks.yaml               # <2>
│   └── firewall.yaml               # <3>
├── content/modules/ROOT/pages/
│   ├── module-01.adoc              # <4>
│   ├── module-02.adoc              # <5>
│   └── module-03.adoc              # <6>
├── runtime-automation/
│   ├── module-01/                  # <7>
│   │   ├── setup-host1.sh
│   │   ├── solve-host1.sh
│   │   └── validation-host1.sh
│   ├── module-02/                  # <8>
│   │   ├── setup-host1.sh
│   │   ├── solve-host1.sh
│   │   └── validation-host1.sh
│   ├── module-03/                  # <9>
│   │   ├── setup-host1.sh
│   │   ├── solve-host1.sh
│   │   └── validation-host1.sh
│   ├── ansible.cfg
│   ├── inventory
│   └── main.yml
├── setup-automation/
│   ├── setup-host1.sh              # <10>
│   ├── ansible.cfg
│   └── main.yml
├── site.yml
└── ui-config.yml
----
<1> **Core Infrastructure**: Defines all VMs and containers
<2> **Network Topology**: Network configuration (default network only in template)
<3> **Security Rules**: Ingress/egress firewall policies (port 443 egress only in template)
<4> **Lab Content**: First module (Writing your lab)
<5> **Second Module**: Antora documentation
<6> **Third Module**: Tabs configuration
<7> **Runtime Scripts**: Automation for `module-01.adoc`
<8> **Runtime Scripts**: Automation for `module-02.adoc`
<9> **Runtime Scripts**: Automation for `module-03.adoc`
<10> **VM Setup**: Script for host1 VM (matches template VM name)

=== Critical Naming Convention Rules

[IMPORTANT]
====
**File Naming Must Match Exactly (Official Template Examples):**

* VM name: `"host1"` → Setup script: `setup-host1.sh`
* Content page: `module-01.adoc` → Runtime automation: `module-01/`
* Content page: `module-02.adoc` → Runtime automation: `module-02/`
* Content page: `module-03.adoc` → Runtime automation: `module-03/`
====

== Configuration File Patterns (Verified Against AgnosticD/AgnosticV Code)

[%collapsible]
====

=== AgnosticV Dynamic Configuration Loading

.Dynamic Config Loading Pattern <<agnosticv-rhel-common>>
[source,yaml]
----
# Git repository URL is converted to raw.githubusercontent.com for direct file access
git_config_directory: |-
  {{ ocp4_workload_showroom_content_git_repo  |
    replace('.git','/refs/heads/' + ocp4_workload_showroom_content_git_repo_ref + '/config/') |
    replace('github.com','raw.githubusercontent.com') }}

# Configuration files are loaded dynamically from git repo
containers: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml', split_lines=False, errors='warn') | default('[]') | from_yaml).containers | default([]) }}"
instances: "{{ (lookup('ansible.builtin.url', git_config_directory + 'instances.yaml', split_lines=False, errors='warn') | default('[]') | from_yaml).virtualmachines | default([]) }}"
networks: "{{ lookup('ansible.builtin.url', git_config_directory + 'networks.yaml', split_lines=False, errors='warn') | default('[]') | from_yaml | default([]) }}"
zero_touch_ingress_lockdown_rules: "{{ (lookup('ansible.builtin.url', git_config_directory + 'firewall.yaml', split_lines=False, errors='warn') | default('{}') | from_yaml).ingress | default([]) }}"
----

=== Ansible Automation Structure

.runtime-automation/main.yml <<template-runtime>>
[source,yaml]
----
---
- name: Create inventory
  hosts: localhost
  gather_facts: false
  tasks:
  - name: Add bastion host
    ansible.builtin.add_host:
      name: "{{ lookup('ansible.builtin.env', 'BASTION_HOST') }}"
      ansible_ssh_host: "{{ lookup('ansible.builtin.env', 'BASTION_HOST') }}"
      ansible_ssh_port: "{{ lookup('ansible.builtin.env', 'BASTION_PORT') }}"
      ansible_ssh_user: "{{ lookup('ansible.builtin.env', 'BASTION_USER') }}"
      ansible_ssh_pass: "{{ lookup('ansible.builtin.env', 'BASTION_PASSWORD') }}"

- name: Demo Playbook for the ansible-runner API
  hosts: all:!localhost
  gather_facts: false
  become: false
  tasks:
  - name: Set config_host variable with ansible_host value
    set_fact:
      config_host: "{{ ansible_host }}"

  - name: Create a directory if it does not exist
    ansible.builtin.file:
      path: "/tmp/setup-scripts/"
      state: directory
      mode: '0755'

  - name: Copy setup script
    ansible.builtin.copy:
      src: "setup-{{ config_host }}.sh"
      dest: "/tmp/setup-scripts/setup-{{ config_host }}.sh"
      mode: '0755'
    ansible.builtin.stat:
      path: "./{{ module_dir }}/{{ module_stage }}-{{ config_host }}.sh"
    delegate_to: localhost
    register: r_script_test

  - name: Execute module script if exists
    when: r_script_test.stat.exists
    shell: "sh -x ./{{ module_dir }}/{{ module_stage }}-{{ ansible_host }}.sh"
    become: true
----

.setup-automation/main.yml  
[source,yaml]
----
---
- name: Create inventory
  hosts: localhost
  gather_facts: false
  tasks:
  - name: Add bastion host
    ansible.builtin.add_host:
      name: "{{ lookup('ansible.builtin.env', 'BASTION_HOST') }}"
      ansible_ssh_host: "{{ lookup('ansible.builtin.env', 'BASTION_HOST') }}"
      ansible_ssh_port: "{{ lookup('ansible.builtin.env', 'BASTION_PORT') }}"
      ansible_ssh_user: "{{ lookup('ansible.builtin.env', 'BASTION_USER') }}"
      ansible_ssh_pass: "{{ lookup('ansible.builtin.env', 'BASTION_PASSWORD') }}"

- name: Initial lab setup
  hosts: all:!localhost
  gather_facts: true
  tasks:
  - name: Set config_host variable
    set_fact:
      config_host: "{{ ansible_host }}"

  - name: Check if setup script exists
    ansible.builtin.stat:
      path: "./setup-{{ config_host }}.sh"
    delegate_to: localhost
    register: r_script_test

  - name: Execute setup script if exists
    when: r_script_test.stat.exists
    shell: "sh -x ./setup-{{ ansible_host }}.sh"
    become: true
----

=== Site Configuration

.site.yml <<template-site>>
[source,yaml]
----
site:
  title: "Zero Touch Platform Training Template"
  url: https://demo.redhat.com/zero-touch-training
  start_page: modules::zero-touch-platform-training.adoc

content:
  sources:
    - url: ./
      start_path: content

ui:
  bundle:
    # CRITICAL: Use actual nookbag bundle, not generic Antora
    url: https://github.com/rhpds/nookbag/releases/download/v0.0.3/ui-bundle.zip
    snapshot: true

output:
  dir: ./www/www

runtime:
  cache_dir: ./.cache/antora
----

NOTE: Current nookbag version in AgnosticV is v0.0.5 <<agnosticv-rhel-common>>

=== UI Configuration & Tab Patterns

.ui-config.yml <<satellite-ui>>
[source,yaml]
----
antora:
  name: modules
  dir: www
  modules:
    - name: "01-introduction"            # <1>
      label: "Introduction"
      solveButton: false
    - name: "02-image-mode"
      label: "Image Mode" 
      solveButton: false
    - name: "03-configure-ansible"
      label: "Configure Ansible"
      solveButton: false

tabs:
  - name: "Satellite Web UI"            # <2>
    url: https://satellite-${guid}.${domain}/
    
  - name: "satellite.lab terminal"      # <3>
    url: /wetty_satellite/ssh/root
    
  - name: "rhel1.lab terminal"          # <4>
    url: /wetty_rhel1/ssh/root
    
  - name: "Satellite 2 Web UI"
    url: https://satellite-2-${guid}.${domain}/
    
  - name: "satellite-2.lab terminal"
    url: /wetty_satellite-2/ssh/root
----
<1> **Module naming**: Use descriptive names matching content structure
<2> **Web applications**: Standard `https://service-${guid}.${domain}/` pattern
<3> **VM terminals**: Pattern is `/wetty_hostname/ssh/user` (NOT `/wetty/ssh/hostname`)
<4> **Hostname format**: Uses `hostname.lab` in tab name, `hostname` in URL path

### **Terminal Inclusion Patterns**

**Source**: Multiple ui-config.yml files across labs

.Method 1: Always Available Terminals <<user-basics-ui>>
[source,yaml]
----
tabs:
  - name: "host1"                         # <1>
    url: /wetty_host1/ssh/root
    # NO modules array = always visible
----
<1> **Global access**: Terminal available across all modules

.Method 2: Per-Module Terminals <<roadshow-ui>>
[source,yaml]
----
tabs:
  - name: aap                             # <2>
    url: https://control-${guid}.${domain}/
    modules:                              # <3>
      - module-01
      - module-02
      - module-03
    external: false
  - name: Report Server
    url: https://node03-${guid}.${domain}/index.html
    external: true
    modules:                              # <4>
     - module-02                          # Only visible in module-02
----
<2> **Conditional visibility**: Tab appears only in specified modules
<3> **Multi-module access**: Tab visible across multiple selected modules
<4> **Single module**: Tab restricted to one specific module

.Console Access Pattern <<upgrades-instances>> <<upgrades-ui>>
[source,yaml]
----
# In instances.yaml
virtualmachines:
  - name: "host"
    consoles:                             # <5>
      - name: console-host
        command: virtctl console host

# In ui-config.yml  
tabs:
  - name: "Serial console"                # <6>
    url: /console-host
----
<5> **Console definition**: Serial console access for VM troubleshooting
<6> **Console access**: Direct VM console for boot issues and debugging

IMPORTANT: 
- **Terminal pattern**: `/wetty_HOSTNAME/ssh/USER` (NOT `/wetty/ssh/hostname`)
- **Console pattern**: `/console-NAME` for direct VM console access
- **Module targeting**: Use `modules:` array to control tab visibility
====

=== Instance Configuration Patterns (Source: zt-ans-bu-roadshow01/config/instances.yaml)

[%collapsible]
====

.Container with Complex Environment (Gitea) <<roadshow-instances>>
[source,yaml]
----
containers:
  - name: gitea
    image: gitea/gitea:1.16.8-rootless
    ports:
      - name: gitea
        containerPort: 3000
        protocol: TCP
    environment:                           # <1>
      GITEA__DEFAULT__RUN_MODE: dev
      GITEA__database__DB_TYPE: sqlite3
      GITEA__database__PATH: /data/gitea/gitea.db
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__service__DISABLE_REGISTRATION: "true"
    volumeMounts:                          # <2>
      - name: gitea-data
        mountPath: /data/
      - name: gitea-etc
        mountPath: /etc/gitea
    volumes:
      - name: gitea-data
        emptyDir: {}
      - name: gitea-etc
        emptyDir: {}
    commands:                              # <3>
      - gitea admin user create --admin --username gitea --password gitea --email dummy@dummy.com --must-change-password=false
      - curl -X POST -H "accept: application/json" -u 'gitea:gitea' -d '{"username": "student"}' http://localhost:3000/api/v1/orgs
    memory: 2Gi
    services:
      - name: gitea
        ports:
          - port: 3000
            targetPort: 3000
    routes:
      - name: gitea
        host: gitea
        service: gitea
        targetPort: 3000
        tls: true
        tls_termination: Edge
----
<1> **Environment variables**: Complex application configuration via env vars
<2> **Volume mounts**: Persistent storage for application data  
<3> **Initialization commands**: API calls and user setup automation

.VM with Enterprise HTTPS (AAP Controller) <<roadshow-instances>>
[source,yaml]
----
virtualmachines:
  - name: "control"
    image: "base-zero-aap-2.5-container-ce"  # <1>
    memory: "16G"
    cores: 4
    image_size: "30Gi"
    tags:
      - key: "AnsibleGroup"
        value: "isolated"                     # <2>
    networks:
      - default
    userdata: |-                             # <3>
      #cloud-config
      user: rhel
      password: ansible123!
      chpasswd: { expire: False }
      runcmd:
        - sed -i "s/PasswordAuthentication no/PasswordAuthentication yes/" /etc/ssh/sshd_config
        - systemctl reload sshd
    services:
      - name: control-https
        ports:
          - port: 443
            targetPort: 443
            name: control-https
    routes:
      - name: control-https
        host: control
        service: control-https
        targetPort: 443
        tls: true
        tls_termination: reencrypt           # <4>
        tls_destinationCACertificate: |      # <5>
          -----BEGIN CERTIFICATE-----
          MIIF1jCCA76gAwIBAgIUBQZlbUZlOmMKhspO9U4/nTJAXAEwDQYJKoZIhvcNAQEL
          [... certificate content ...]
          -----END CERTIFICATE-----
----
<1> **Custom images**: Pre-built images with enterprise applications
<2> **Ansible grouping**: Tags for automation targeting
<3> **Cloud-init userdata**: Automated VM configuration on boot
<4> **TLS reencrypt**: Enterprise pattern for applications with existing TLS
<5> **Custom certificates**: Application-specific TLS certificates

.VM with Simple Web Service (RHEL Node) <<roadshow-instances>>
[source,yaml]
----
virtualmachines:
  - name: "node02"
    image: "rhel-8.7"                      # <1>
    memory: "2G"
    cores: 2
    image_size: "30Gi"
    services:
      - name: node02-http
        ports:
          - port: 80
            targetPort: 80
            name: node02
    routes:
      - name: node02-web
        host: node02
        service: node02-http
        targetPort: 80
        tls: true                          # <2>
        tls_termination: Edge              # <3>
----
<1> **Multi-version support**: Different RHEL versions for testing scenarios
<2> **HTTPS enforcement**: Force TLS even for HTTP applications
<3> **Edge termination**: Simple TLS termination at the route level

.Windows VM Configuration <<roadshow-instances>>
[source,yaml]
----
virtualmachines:
  - name: "windows"
    image: "base-windows-ad-2022"
    memory: "16G"                          # <1>
    cores: 4
    image_size: "60Gi"
    interface_model: "e1000e"              # <2>
    services:
      - name: windows-rdp                  # <3>
        ports:
          - port: 3389
            targetPort: 3389
            name: windows-rdp
      - name: iis                          # <4>
        ports:
          - port: 80
            targetPort: 80
            name: iis
    routes:
      - name: windows
        host: windows
        service: iis
        targetPort: 80
        tls: true
        tls_termination: Edge
----
<1> **Higher resource allocation**: Windows typically requires more memory
<2> **Network interface model**: Specific driver for Windows compatibility  
<3> **RDP access**: Remote desktop protocol for Windows management
<4> **Multiple services**: Both management (RDP) and application (IIS) services

.Runtime Script Naming Pattern
[source,bash]
----
# runtime-automation/module-01/ directory structure
runtime-automation/
  module-01/
    ├── setup-control.sh         # module_stage=setup, config_host=control
    ├── solve-control.sh          # module_stage=solve, config_host=control  
    ├── validation-control.sh     # module_stage=validation, config_host=control
    ├── setup-node01.sh          # Scripts for different hosts
    └── solve-node01.sh

# Variables used by main.yml:
# module_dir = "module-01" 
# module_stage = "setup" | "solve" | "validation"
# config_host = hostname from instances.yaml (e.g., "control", "node01")
----
====

== Code Verification Summary

[%collapsible]
====

### **All Patterns Verified Against Actual AgnosticD/AgnosticV Code**

| Pattern | Source File | Line(s) | Status |
|---------|-------------|---------|---------|
| **Dynamic Git Loading** | `zt-rhel-bu-lab-developer-cnv/common.yaml` | 13-22 |  **VERIFIED** |
| **Terminal URL Pattern** | `zt-satellite-advanced-topics-6-17/ui-config.yml` | 35-39 |  **VERIFIED** |
| **Nookbag Bundle** | `zt-rhel-bu-lab-developer-cnv/common.yaml` | 32 |  **VERIFIED** |
| **Runtime main.yml** | `zero_touch_template_wilson/runtime-automation/main.yml` | 1-73 |  **VERIFIED** |
| **Setup main.yml** | `zero_touch_template_wilson/setup-automation/main.yml` | 1-76 |  **VERIFIED** |
| **Network Policies** | `zero-touch-base-rhel/default_vars_openshift_cnv.yaml` | 65-90 |  **VERIFIED** |
| **Service Exposure** | `zt-ans-bu-roadshow01/config/instances.yaml` | 88-135 |  **VERIFIED** |

### **Critical Corrections Made:**

1. **Terminal Pattern**: Changed from `/wetty/ssh/hostname` to `/wetty_hostname/ssh/user`
2. **UI Config**: Simplified to actual tab array format without modules restrictions  
3. **Git Loading**: Added dynamic URL lookup patterns from AgnosticV
4. **Network Security**: Added actual SSH restriction patterns from AgnosticD
5. **Bundle Version**: Corrected to nookbag v0.0.5 (current production version)

### **Repository References:**
- **AgnosticD**: Git repository - ansible/configs/zero-touch-base-rhel/
- **AgnosticV**: Git repository - zt-rhelbu-agnosticv/zt-rhelbu/
- **Production Labs**: Git repository - converted lab configurations

TIP: All patterns in this guide are now verified against the actual backend implementation and will work in production deployments.
====

== Hands-On Exercise: Production-Level Multi-VM + Container Lab

[%collapsible]
====

**Source**: Based on `zt-ans-bu-roadshow01` (AAP roadshow) configuration patterns

This exercise demonstrates enterprise-level lab architecture with multiple VMs, containers, and service exposure patterns commonly used in Red Hat demonstrations.

.Lab Architecture Overview
[source,text]
----
┌─────────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐
│  Control Node       │    │  RHEL Nodes         │    │  Gitea Container    │
│  (AAP Controller)   │    │  (Web Services)     │    │  (Source Control)   │
│                     │    │                     │    │                     │
│  • RHEL 9.5         │    │  • RHEL 8.7/9.5     │    │  • API setup       │
│  • 16G RAM, 4 CPU   │    │  • 2G RAM, 2 CPU    │    │  • User automation  │
│  • TLS reencrypt    │    │  • Edge termination │    │  • Repo migration   │
│  • Custom certs     │    │  • HTTP services    │    │  • Volume mounts    │
└─────────────────────┘    └─────────────────────┘    └─────────────────────┘
                │                       │                       │
                └───────────────────────┼───────────────────────┘
                                        │
                            ┌─────────────────────┐
                            │  Windows Server     │
                            │  (Mixed Environment)│
                            │                     │
                            │  • Windows 2022     │
                            │  • 16G RAM, 4 CPU   │
                            │  • RDP + IIS        │
                            │  • Multi-service    │
                            └─────────────────────┘
----

=== Step 1: Create Production instances.yaml

**Source**: Verified against `zt-ans-bu-roadshow01/config/instances.yaml`

[source,yaml]
.config/instances.yaml <<roadshow-instances>>
----
---
containers:
  - name: gitea                               # <1>
    image: gitea/gitea:1.16.8-rootless
    ports:
      - name: gitea
        containerPort: 3000
        protocol: TCP
    environment:                              # <2>
      GITEA__DEFAULT__RUN_MODE: dev
      GITEA__database__DB_TYPE: sqlite3
      GITEA__database__PATH: /data/gitea/gitea.db
      GITEA__picture__DISABLE_GRAVATAR: "true"
      GITEA__repository__DEFAULT_PRIVATE: "false"
      GITEA__repository__ENABLE_PUSH_CREATE_USER: "true"
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__service__DISABLE_REGISTRATION: "true"
      GITEA__webhook__ALLOWED_HOST_LIST: '*'
    volumeMounts:
      - name: gitea-data
        mountPath: /data/
      - name: gitea-etc
        mountPath: /etc/gitea
    volumes:
      - name: gitea-data
        emptyDir: {}
      - name: gitea-etc
        emptyDir: {}
    commands:                                 # <3>
      - gitea admin user create --admin --username gitea --password gitea --email dummy@dummy.com --must-change-password=false
      - curl -X POST -H "accept: application/json" -H "Content-Type: application/json" -u 'gitea:gitea' -d '{"username": "student", "full_name": "student"}' http://localhost:3000/api/v1/orgs
      - curl -X POST -u 'gitea:gitea' -d '{"clone_addr": "https://github.com/ansible-tmm/aap25-roadshow", "repo_name": "aap25-roadshow-content", "owner": "student", "uid": 2, "private": false}' http://localhost:3000/api/v1/repos/migrate
    memory: 2Gi
    services:
      - name: gitea
        ports:
          - port: 3000
            targetPort: 3000
    routes:
      - name: gitea
        host: gitea
        service: gitea
        targetPort: 3000
        tls: true
        tls_termination: Edge

virtualmachines:
  - name: "control"                          # <4>
    image: "base-zero-aap-2.5-container-ce"
    memory: "16G"
    cores: 4
    image_size: "30Gi"
    tags:
      - key: "AnsibleGroup"
        value: "isolated"
    networks:
      - default
    userdata: |-                             # <5>
      #cloud-config
      user: rhel
      password: ansible123!
      chpasswd: { expire: False }
      runcmd:
        - sed -i "s/PasswordAuthentication no/PasswordAuthentication yes/" /etc/ssh/sshd_config
        - systemctl reload sshd
    services:
      - name: control-https
        ports:
          - port: 443
            targetPort: 443
            name: control-https
    routes:
      - name: control-https
        host: control
        service: control-https
        targetPort: 443
        tls: true
        tls_termination: reencrypt            # <6>
        
  - name: "node01"                           # <7>
    image: "rhel-9.5"
    memory: "2G"
    cores: 2
    image_size: "30Gi"
    tags:
      - key: "AnsibleGroup"
        value: "isolated"
    networks:
      - default
    userdata: |-
      #cloud-config
      user: rhel
      password: ansible123!
      chpasswd: { expire: False }  
      runcmd:
        - echo "PasswordAuthentication yes" > /etc/ssh/sshd_config.d/50-cloud-init.conf
        - systemctl reload sshd

  - name: "node02"                           # <8>
    image: "rhel-8.7"
    memory: "2G"
    cores: 2
    image_size: "30Gi"
    services:
      - name: node02-http
        ports:
          - port: 80
            targetPort: 80
            name: node02
    routes:
      - name: node02-web
        host: node02
        service: node02-http
        targetPort: 80
        tls: true
        tls_termination: Edge
    userdata: |-
      #cloud-config
      user: rhel
      password: ansible123!
      chpasswd: { expire: False }
      runcmd:
        - sed -i "s/PasswordAuthentication no/PasswordAuthentication yes/" /etc/ssh/sshd_config
        - systemctl reload sshd
----
<1> **Container first**: Gitea provides source control for the lab
<2> **Production config**: Realistic Gitea settings with security controls
<3> **Automation commands**: API-driven user and repository setup
<4> **Control node**: AAP controller with high resource allocation
<5> **Cloud-init**: Automated SSH configuration on all VMs
<6> **Enterprise TLS**: Reencrypt for applications with existing certificates
<7> **Worker nodes**: Multiple RHEL versions for compatibility testing
<8> **Web services**: HTTP services with TLS edge termination

=== Step 2: Configure UI with Production Patterns

**Source**: Verified against multiple ui-config.yml files

[source,yaml]
.ui-config.yml <<roadshow-ui>>
----
---
antora:
  name: modules
  dir: www
  modules:
    - name: module-01
      label: "Backup and Snapshots"
      solveButton: false
    - name: module-02
      label: "Infrastructure Reporting" 
      solveButton: false
    - name: module-03
      label: "Windows Reporting"
      solveButton: false

tabs:
  - name: aap                              # <1>
    url: https://control-${guid}.${domain}/
    modules:                               # <2>
      - module-01
      - module-02
      - module-03
    external: false
    
  - name: Report Server                    # <3>
    url: https://node02-${guid}.${domain}/index.html
    external: true
    modules:
     - module-02                           # <4>
     
  - name: Gitea                           # <5>
    url: https://gitea-${guid}.${domain}/
    external: true
    
  # Terminal access patterns                # <6>
  # - name: ">_ control"
  #   url: /wetty_control/ssh/rhel          # Always available
  # - name: ">_ node01"  
  #   url: /wetty_node01/ssh/rhel
  # - name: ">_ node02"
  #   url: /wetty_node02/ssh/rhel
----
<1> **Primary application**: AAP controller visible across all modules
<2> **Module targeting**: Specify which modules show each tab
<3> **Per-module services**: Services specific to certain lab sections
<4> **Single module access**: Tab only visible in module-02
<5> **Global services**: Available throughout the lab (no modules array)
<6> **Terminal access**: Commented examples showing naming patterns

### **Two Terminal Access Methods**

**Method A: Always Available** <<user-basics-ui>>
[source,yaml]
----
tabs:
  - name: "host1"
    url: /wetty_host1/ssh/root             # No modules = always visible
----

**Method B: Module-Specific** <<roadshow-ui>>  
[source,yaml]
----
tabs:
  - name: ">_ control"
    url: /wetty_control/ssh/rhel
    modules: [module-01, module-02]        # Only visible in specified modules
----

=== Step 3: Network and Console Patterns

**Source**: `zt-in-place-upgrades-9` for console access patterns

.Console Access for VM Troubleshooting
[source,yaml]
----
# In instances.yaml
virtualmachines:
  - name: "host"
    consoles:                             # <1>
      - name: console-host
        command: virtctl console host

# In ui-config.yml  
tabs:
  - name: "Serial console"                # <2>
    url: /console-host
----
<1> **Console definition**: Serial console access for VM troubleshooting
<2> **Console access**: Direct VM console for boot issues and debugging

=== Step 3: Create Lab Content

.Quick Development Module Example
[source,asciidoc]
----
= Module 1: Development Environment Setup
:estimated-time: 10 minutes

== Learning Objectives

By the end of this module, you will:

* Access your development server via SSH
* Configure Git for lab development  
* Deploy a test application using Gitea
* Understand the development workflow

== Step 1: Access Development Environment

Connect to your development server:

[source,bash,role=execute]
----
ssh rhel@dev-server
----

== Step 2: Access Gitea Web Interface

Open Gitea: https://gitea[Gitea^,role=params-link]

Login Credentials:
- Username: gitea
- Password: gitea

== Validation

Verify your setup:

[source,bash,role=execute] 
----
systemctl is-active sshd podman.socket
curl -f https://gitea/api/v1/version
----
----

=== Step 4: Deploy and Test

.Deployment Commands
[source,bash]
----
# Validate configuration
yamllint config/instances.yaml

# Deploy lab (example command - actual deployment via AgnosticD)
# This would be handled by the platform deployment system
echo "Configuration ready for Zero Touch deployment"

# Test local content rendering (if Antora available)
antora generate --stacktrace site.yml
----

==  Enterprise Configuration Patterns

[%collapsible]
====
Based on production Satellite and Ansible labs, here are advanced patterns for enterprise deployments.

=== High-Performance VM Configuration

.Enterprise Satellite Pattern (from zt-satellite-advanced-topics-6-17)
[source,yaml]
----
virtualmachines:
  - name: "satellite"
    image: "satellite-server-rhdp-1-6-17-07-02-25"
    bootloader: efi
    register_satellite: false
    memory: "32G"
    cores: 8
    image_size: "540G"
    tags:
      - key: "AnsibleGroup"
        value: "bastions"
    networks:
      - default
    userdata: |
      #cloud-config
      fqdn: satellite.lab
      hostname: satellite
      prefer_fqdn_over_hostname: true
      manage_etc_hosts: true
    services:
      - name: satellite-https
        ports:
          - port: 443
            protocol: TCP
            targetPort: 443
            name: satellite-https
    routes:
      - name: satellite-https
        host: satellite
        service: satellite-https
        targetPort: 443
        tls: true
        tls_termination: reencrypt
        tls_destinationCACertificate: |
          -----BEGIN CERTIFICATE-----
          MIIGyTCCBLGgAwIBAgIUZPQwm28Gh5V2T/aZvVelkHZYRB4wDQYJKoZIhvcNAQEL
          # ... (certificate content truncated for brevity)
          -----END CERTIFICATE-----
----

=== Multi-Platform Environment Pattern

.Mixed Linux/Windows/Container Pattern (from zt-ans-bu-roadshow01)
[source,yaml]
----
# Container for development tools
containers:
  - name: "gitea"
    image: "gitea/gitea:1.16.8-rootless"
    # ... (container configuration)

virtualmachines:
  # Control node with Ansible Automation Platform
  - name: "control"
    image: "base-zero-aap-2.5-container-ce"
    memory: "16G"
    cores: 4
    
  # Multiple Linux versions for testing
  - name: "node01"
    image: "rhel-9.5"
    memory: "2G"
    cores: 2
    
  - name: "node02"
    image: "rhel-8.7"
    memory: "2G"  
    cores: 2
    services:
      - name: node02-http
        ports:
          - port: 80
            protocol: TCP
            targetPort: 80
    
  # Windows environment for hybrid automation
  - name: "windows"
    image: "base-windows-ad-2022"
    memory: "16G"
    cores: 4
    image_size: "60Gi"
    interface_model: "e1000e"
    services:
      - name: windows-rdp
        ports:
          - port: 3389
            protocol: TCP
            targetPort: 3389
----

=== Advanced Storage and Networking

.Enterprise Storage Pattern
[source,yaml]
----
virtualmachines:
  - name: "database-server"
    memory: "16G"
    cores: 8
    additional_disks:
      - name: "data-disk"
        size: "500G"
        type: "ssd"
      - name: "log-disk" 
        size: "100G"
        type: "ssd"
      - name: "backup-disk"
        size: "1T"
        type: "hdd"
    interface_model: "virtio"
    networks:
      - default
      - storage-network
      - backup-network
----
====

==  Common Enterprise Patterns Summary

[%collapsible]
====
.Enterprise Configuration Checklist
[cols="2,1,3"]
|===
|Pattern |Usage |Key Configuration

|**High-Memory Applications**
|Satellite, AAP, Databases
|`memory: "32G"`, `cores: 8`, `bootloader: efi`

|**Multi-Platform Testing**
|CI/CD, Compatibility Testing  
|Multiple RHEL versions + Windows, `interface_model: "e1000e"`

|**Certificate Management**
|Production Applications
|`tls_termination: reencrypt`, custom CA certificates

|**Storage Optimization**
|Database Workloads
|Multiple disks, SSD for performance, network segmentation

|**Container Integration**
|Development Environments
|Gitea, Registry, CI tools with proper volume mounts

|**Network Segmentation**
|Security Requirements
|Multiple networks, firewall rules, service isolation
|===
====

== Related Documentation

[%collapsible]
====
* xref:advanced-lab-features.adoc[Advanced Lab Features Guide]
* xref:enterprise-lab-patterns.adoc[Enterprise Lab Patterns]  
* xref:advanced-troubleshooting-operations.adoc[Advanced Troubleshooting & Operations]
* xref:template-customization-guide.adoc[Template Customization Guide]
====

[bibliography]
== References

* [[[agnosticd-base]]] Red Hat GPTE Team. AgnosticD Zero Touch Base RHEL Configuration. 
  AgnosticD Git Repository - ansible/configs/zero-touch-base-rhel/default_vars_openshift_cnv.yaml. 2024.

* [[[agnosticd-software]]] Red Hat GPTE Team. AgnosticD Zero Touch Software Configuration. 
  AgnosticD Git Repository - ansible/configs/zero-touch-base-rhel/software.yml. 2024.

* [[[agnosticv-rhel-common]]] Red Hat GPTE Team. AgnosticV RHEL BU Lab Developer CNV Common Configuration. 
  AgnosticV Git Repository - zt-rhelbu-agnosticv/zt-rhelbu/zt-rhel-bu-lab-developer-cnv/common.yaml. 2024.

* [[[roadshow-instances]]] Red Hat Ansible Team. AAP 2.5 Roadshow Lab Instance Configuration. 
  AgnosticV Git Repository - zt-ans-bu-roadshow01/config/instances.yaml. 2024.

* [[[roadshow-ui]]] Red Hat Ansible Team. AAP 2.5 Roadshow Lab UI Configuration. 
  AgnosticV Git Repository - zt-ans-bu-roadshow01/ui-config.yml. 2024.

* [[[roadshow-runtime]]] Red Hat Ansible Team. AAP 2.5 Roadshow Lab Runtime Automation. 
  AgnosticV Git Repository - zt-ans-bu-roadshow01/runtime-automation/main.yml. 2024.

* [[[roadshow-setup]]] Red Hat Ansible Team. AAP 2.5 Roadshow Lab Setup Automation. 
  AgnosticV Git Repository - zt-ans-bu-roadshow01/setup-automation/main.yml. 2024.

* [[[satellite-ui]]] Red Hat Satellite Team. Satellite Advanced Topics 6.17 UI Configuration. 
  AgnosticV Git Repository - zt-satellite-advanced-topics-6-17/ui-config.yml. 2024.

* [[[user-basics-ui]]] Red Hat RHEL Team. Managing User Basics UI Configuration. 
  AgnosticV Git Repository - zt-managing-user-basics/ui-config.yml. 2024.

* [[[upgrades-ui]]] Red Hat RHEL Team. In-Place Upgrades RHEL 9 UI Configuration. 
  AgnosticV Git Repository - zt-in-place-upgrades-9/ui-config.yml. 2024.

* [[[upgrades-instances]]] Red Hat RHEL Team. In-Place Upgrades RHEL 9 Instance Configuration. 
  AgnosticV Git Repository - zt-in-place-upgrades-9/config/instances.yaml. 2024.

* [[[navigator-ui]]] Red Hat Ansible Team. Ansible Navigator Getting Started UI Configuration. 
  AgnosticV Git Repository - zt-get-started-ansible-navigator/ui-config.yml. 2024.

* [[[template-site]]] Red Hat GPTE Team. Zero Touch Template Site Configuration. 
  `https://github.com/rhpds/lab_zero_touch_template.git` - site.yml. 2024.

* [[[template-instances]]] Red Hat GPTE Team. Zero Touch Template Instance Configuration. 
  `https://github.com/rhpds/lab_zero_touch_template.git` - config/instances.yaml. 2024.

* [[[template-runtime]]] Red Hat GPTE Team. Zero Touch Template Runtime Automation. 
  `https://github.com/rhpds/lab_zero_touch_template.git` - runtime-automation/main.yml. 2024.

* [[[template-setup]]] Red Hat GPTE Team. Zero Touch Template Setup Automation. 
  `https://github.com/rhpds/lab_zero_touch_template.git` - setup-automation/main.yml. 2024.
